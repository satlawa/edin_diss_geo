{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NumbersDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'root_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4ae1062498a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNumbersDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m122\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m361\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'root_dir'"
     ]
    }
   ],
   "source": [
    "dataset = NumbersDataset()\n",
    "print(len(dataset))\n",
    "print(dataset[100])\n",
    "print(dataset[122:361])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import splitext\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, imgs_dir, masks_dir, scale=1, mask_suffix=''):\n",
    "        \n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.scale = scale\n",
    "        self.mask_suffix = mask_suffix\n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    "\n",
    "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        idx = self.ids[i]\n",
    "        mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\n",
    "        img_file = glob(self.imgs_dir + idx + '.*')\n",
    "\n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    "\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
    "\n",
    "        img = self.preprocess(img, self.scale)\n",
    "        mask = self.preprocess(mask, self.scale)\n",
    "\n",
    "        return {\n",
    "            'image': torch.from_numpy(img).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file_name  = \"/media/philipp/ed7d22ba-5a3b-4d31-bf6c-6add6e106b3d/test/256x256/1m/dataset_256.hdf5\";\n",
    "x = 'ortho'\n",
    "y = 'ground_truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from hdf5\n",
    "def read_hdf5(hdf5_file_name):\n",
    "    \"\"\" Reads image from HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        num_images   number of images to read\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images      images array, (N, 32, 32, 3) to be stored\n",
    "        labels      associated meta data, int label (N, 1)\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(hdf5_file_name, \"r+\")\n",
    "\n",
    "    images = np.array(file[\"/ortho\"]).astype(\"uint8\")\n",
    "    labels = np.array(file[\"/ground_truth\"]).astype(\"uint8\")\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = read_hdf5(ds_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20928, 256, 256, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_name, scale=1):\n",
    "        \n",
    "        self.hdf5_file_name = file_name\n",
    "        self.scale = scale\n",
    "        \n",
    "        # load dataset\n",
    "        self.images, self.labels = self.read_hdf5(self.hdf5_file_name)\n",
    "        \n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    "        logging.info(f'Creating dataset with {len(self.images)} examples')\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.images)\n",
    "    \n",
    "    @classmethod\n",
    "    def preprocess(cls, img_nd):\n",
    "\n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    "\n",
    "        # HWC to CHW\n",
    "        img_trans = img_nd.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "\n",
    "        return img_trans\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        mask = self.labels[i]\n",
    "        img = self.images[i]\n",
    "\n",
    "        #assert img.size == mask.size, \\\n",
    "        #    f'Image and mask should be the same size, but are {img[:,:,1].size} and {mask.size}'\n",
    "\n",
    "        img = self.preprocess(img)\n",
    "        mask = self.preprocess(mask)\n",
    "\n",
    "        return {\n",
    "            'image': torch.from_numpy(img).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
    "        }\n",
    "    \n",
    "    def read_hdf5(self, hdf5_file_name):\n",
    "        \"\"\" Reads image from HDF5.\n",
    "            Parameters:\n",
    "            ---------------\n",
    "            num_images   number of images to read\n",
    "\n",
    "            Returns:\n",
    "            ----------\n",
    "            images      images array, (N, 32, 32, 3) to be stored\n",
    "            labels      associated meta data, int label (N, 1)\n",
    "        \"\"\"\n",
    "        images, labels = [], []\n",
    "\n",
    "        # Open the HDF5 file\n",
    "        file = h5py.File(hdf5_file_name, \"r+\")\n",
    "\n",
    "        images = np.array(file[\"/ortho\"]).astype(\"uint8\")\n",
    "        labels = np.array(file[\"/ground_truth\"]).astype(\"uint8\")\n",
    "\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BasicDataset(ds_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[0.3882, 0.4392, 0.3765,  ..., 0.5608, 0.5725, 0.5529],\n",
       "          [0.3922, 0.3843, 0.3843,  ..., 0.5294, 0.5412, 0.5529],\n",
       "          [0.3725, 0.3961, 0.3804,  ..., 0.5725, 0.5725, 0.5804],\n",
       "          ...,\n",
       "          [0.2588, 0.2588, 0.2588,  ..., 0.2667, 0.3725, 0.4745],\n",
       "          [0.2471, 0.2549, 0.2471,  ..., 0.3804, 0.4392, 0.4078],\n",
       "          [0.2549, 0.2549, 0.2627,  ..., 0.4235, 0.3765, 0.4431]],\n",
       " \n",
       "         [[0.3922, 0.4588, 0.3765,  ..., 0.5843, 0.6000, 0.5765],\n",
       "          [0.4039, 0.3922, 0.3882,  ..., 0.5529, 0.5725, 0.5765],\n",
       "          [0.3804, 0.4039, 0.3882,  ..., 0.6078, 0.6039, 0.6039],\n",
       "          ...,\n",
       "          [0.2627, 0.2745, 0.2902,  ..., 0.2745, 0.3882, 0.5176],\n",
       "          [0.2588, 0.2745, 0.2745,  ..., 0.4000, 0.4784, 0.4078],\n",
       "          [0.2667, 0.2784, 0.2863,  ..., 0.4588, 0.3843, 0.4706]],\n",
       " \n",
       "         [[0.3176, 0.3882, 0.2941,  ..., 0.4745, 0.5059, 0.4667],\n",
       "          [0.3216, 0.3098, 0.3020,  ..., 0.4275, 0.4471, 0.4627],\n",
       "          [0.3059, 0.3255, 0.3137,  ..., 0.5020, 0.4941, 0.5059],\n",
       "          ...,\n",
       "          [0.2627, 0.2667, 0.2824,  ..., 0.2392, 0.3333, 0.4471],\n",
       "          [0.2549, 0.2706, 0.2706,  ..., 0.3294, 0.4000, 0.3059],\n",
       "          [0.2588, 0.2706, 0.2902,  ..., 0.3608, 0.2824, 0.3765]],\n",
       " \n",
       "         [[0.7843, 0.8157, 0.7686,  ..., 0.8157, 0.8275, 0.8157],\n",
       "          [0.7961, 0.7882, 0.7882,  ..., 0.8000, 0.8078, 0.8157],\n",
       "          [0.7804, 0.8000, 0.7882,  ..., 0.8275, 0.8275, 0.8275],\n",
       "          ...,\n",
       "          [0.4275, 0.4706, 0.5373,  ..., 0.6706, 0.7529, 0.8235],\n",
       "          [0.4353, 0.4824, 0.5176,  ..., 0.7647, 0.8078, 0.7725],\n",
       "          [0.4275, 0.4745, 0.5333,  ..., 0.7961, 0.7608, 0.8118]]]),\n",
       " 'mask': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 0.,  ..., 0., 0., 0.]]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_mask(img, mask):\n",
    "    classes = mask.shape[2] if len(mask.shape) > 2 else 1\n",
    "    fig, ax = plt.subplots(1, classes + 1)\n",
    "    ax[0].set_title('Input image')\n",
    "    ax[0].imshow(img)\n",
    "    if classes > 1:\n",
    "        for i in range(classes):\n",
    "            ax[i+1].set_title(f'Output mask (class {i+1})')\n",
    "            ax[i+1].imshow(mask[:, :, i])\n",
    "    else:\n",
    "        ax[1].set_title(f'Output mask')\n",
    "        ax[1].imshow(mask)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
