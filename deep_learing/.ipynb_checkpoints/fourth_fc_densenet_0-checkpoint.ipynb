{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "SAVE_PATH = 'model/unet_ba_2.pt'\n",
    "\n",
    "# set device\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForestDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    '''Characterizes a dataset for PyTorch'''\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        '''Initialization'''\n",
    "        # open dataset\n",
    "        self.dset = h5py.File(path, 'r')\n",
    "        self.ortho = self.dset['x_ortho']\n",
    "        #self.dsm = self.dset['x_dsm']\n",
    "        #self.dtm = self.dset['x_dtm']\n",
    "        #self.slope = self.dset['x_slope']\n",
    "        self.ground_truth = self.dset['x_ground_truth']\n",
    "        \n",
    "        # set number of samples\n",
    "        self.length = self.ground_truth.shape[0]\n",
    "        #self.labels = labels\n",
    "        #self.list_IDs = list_IDs\n",
    "        \n",
    "        ## TODO:\n",
    "        # make means and stds load from hdf5\n",
    "        self.means = np.array([106.623276, 14.312894, 1325.6674, 29.272326])\n",
    "        self.stds = np.array([54.006954, 12.487458, 16.26564, 7.691262])\n",
    "        \n",
    "        #self.transforms = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Denotes the total number of samples'''\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Generates one sample of data'''\n",
    "        \n",
    "        # Load data and get label\n",
    "        X = torch.tensor(self.ortho[index], dtype=torch.float32).permute(2, 0, 1) / 255\n",
    "        y = torch.tensor(self.ground_truth[index][:,:,0], dtype=torch.torch.int64)\n",
    "        \n",
    "        #X = torch.from_numpy(np.array(self.ortho[index]).transpose(2,0,1)) / 255\n",
    "        #y = torch.from_numpy(np.array(self.ground_truth[index])[:,:,0], dtype=torch.torch.int64)\n",
    "\n",
    "        return X, y #torch.from_numpy(y).permute(2, 0, 1)\n",
    "        \n",
    "        \n",
    "    def show_item(self, index):\n",
    "        '''shows the data'''\n",
    "        #plt.imshow(np.array(self.ground_truth[index]))\n",
    "        \n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "        \n",
    "        a = fig.add_subplot(1, 3, 1)\n",
    "        imgplot = plt.imshow(np.array(self.ortho[index][:,:,:3]))\n",
    "        a.set_title('RGB')\n",
    "        plt.colorbar(ticks=[0.1, 0.3, 0.5, 0.7], orientation='horizontal')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        a = fig.add_subplot(1, 3, 2)\n",
    "        imgplot = plt.imshow(np.array(np.roll(self.ortho[index], 1, axis=2)[:,:,:3]))\n",
    "        a.set_title('CIR')\n",
    "        plt.colorbar(ticks=[0.1, 0.3, 0.5, 0.7], orientation='horizontal')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        a = fig.add_subplot(1, 3, 3)\n",
    "        imgplot = plt.imshow(np.array(self.ground_truth[index]), cmap=\"hot\")\n",
    "        imgplot.set_clim(0, 7)\n",
    "        a.set_title('Ground Truth')\n",
    "        plt.colorbar(ticks=[1, 2, 3, 4, 5, 6], orientation='horizontal')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        ''' closes the hdf5 file'''\n",
    "        self.dset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = \"/home/philipp/Data/dataset/dataset_256_ba.h5\"\n",
    "data = ForestDataset(path_dataset)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[1000]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[51]\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,9))\n",
    "ax[0].imshow(X.permute(1, 2, 0).numpy()[:,:,:3])\n",
    "ax[1].imshow(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "validation_split = .1\n",
    "test_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed = 12\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = 91000\n",
    "#dataset_size = len(data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices, test_indices = indices[2*split:], indices[:split], indices[split:2*split]\n",
    "\n",
    "print(len(train_indices))\n",
    "print(len(val_indices))\n",
    "print(len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = sampler.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = sampler.SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(data, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=4, pin_memory=True)\n",
    "valid_dl = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FCDenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super().__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(in_channels))\n",
    "        self.add_module('relu', nn.ReLU(True))\n",
    "        self.add_module('conv', nn.Conv2d(in_channels, growth_rate, kernel_size=3,\n",
    "                                          stride=1, padding=1, bias=True))\n",
    "        self.add_module('drop', nn.Dropout2d(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        self.layers = nn.ModuleList([DenseLayer(\n",
    "            in_channels + i*growth_rate, growth_rate)\n",
    "            for i in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            new_features = []\n",
    "            #we pass all previous activations into each dense layer normally\n",
    "            #But we only store each dense layer's output in the new_features array\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1)\n",
    "                new_features.append(out)\n",
    "            return torch.cat(new_features,1)\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1) # 1 = channel axis\n",
    "            return x\n",
    "\n",
    "\n",
    "class TransitionDown(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(in_channels, in_channels,\n",
    "                                          kernel_size=1, stride=1,\n",
    "                                          padding=0, bias=True))\n",
    "        self.add_module('drop', nn.Dropout2d(0.2))\n",
    "        self.add_module('maxpool', nn.MaxPool2d(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class TransitionUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convTrans = nn.ConvTranspose2d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=3, stride=2, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        out = self.convTrans(x)\n",
    "        out = center_crop(out, skip.size(2), skip.size(3))\n",
    "        out = torch.cat([out, skip], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers):\n",
    "        super().__init__()\n",
    "        self.add_module('bottleneck', DenseBlock(\n",
    "            in_channels, growth_rate, n_layers, upsample=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "def center_crop(layer, max_height, max_width):\n",
    "    _, _, h, w = layer.size()\n",
    "    xy1 = (w - max_width) // 2\n",
    "    xy2 = (h - max_height) // 2\n",
    "    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDenseNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, down_blocks=(5,5,5,5,5),\n",
    "                 up_blocks=(5,5,5,5,5), bottleneck_layers=5,\n",
    "                 growth_rate=16, out_chans_first_conv=48, n_classes=12):\n",
    "        super().__init__()\n",
    "        self.down_blocks = down_blocks\n",
    "        self.up_blocks = up_blocks\n",
    "        cur_channels_count = 0\n",
    "        skip_connection_channel_counts = []\n",
    "\n",
    "        ## First Convolution ##\n",
    "\n",
    "        self.add_module('firstconv', nn.Conv2d(in_channels=in_channels,\n",
    "                  out_channels=out_chans_first_conv, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True))\n",
    "        cur_channels_count = out_chans_first_conv\n",
    "\n",
    "        #####################\n",
    "        # Downsampling path #\n",
    "        #####################\n",
    "\n",
    "        self.denseBlocksDown = nn.ModuleList([])\n",
    "        self.transDownBlocks = nn.ModuleList([])\n",
    "        for i in range(len(down_blocks)):\n",
    "            self.denseBlocksDown.append(\n",
    "                DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n",
    "            cur_channels_count += (growth_rate*down_blocks[i])\n",
    "            skip_connection_channel_counts.insert(0,cur_channels_count)\n",
    "            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n",
    "\n",
    "        #####################\n",
    "        #     Bottleneck    #\n",
    "        #####################\n",
    "\n",
    "        self.add_module('bottleneck',Bottleneck(cur_channels_count,\n",
    "                                     growth_rate, bottleneck_layers))\n",
    "        prev_block_channels = growth_rate*bottleneck_layers\n",
    "        cur_channels_count += prev_block_channels\n",
    "\n",
    "        #######################\n",
    "        #   Upsampling path   #\n",
    "        #######################\n",
    "\n",
    "        self.transUpBlocks = nn.ModuleList([])\n",
    "        self.denseBlocksUp = nn.ModuleList([])\n",
    "        for i in range(len(up_blocks)-1):\n",
    "            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n",
    "            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n",
    "\n",
    "            self.denseBlocksUp.append(DenseBlock(\n",
    "                cur_channels_count, growth_rate, up_blocks[i],\n",
    "                    upsample=True))\n",
    "            prev_block_channels = growth_rate*up_blocks[i]\n",
    "            cur_channels_count += prev_block_channels\n",
    "\n",
    "        ## Final DenseBlock ##\n",
    "\n",
    "        self.transUpBlocks.append(TransitionUp(\n",
    "            prev_block_channels, prev_block_channels))\n",
    "        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n",
    "\n",
    "        self.denseBlocksUp.append(DenseBlock(\n",
    "            cur_channels_count, growth_rate, up_blocks[-1],\n",
    "                upsample=False))\n",
    "        cur_channels_count += growth_rate*up_blocks[-1]\n",
    "\n",
    "        ## Softmax ##\n",
    "\n",
    "        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n",
    "               out_channels=n_classes, kernel_size=1, stride=1,\n",
    "                   padding=0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.firstconv(x)\n",
    "\n",
    "        skip_connections = []\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            out = self.denseBlocksDown[i](out)\n",
    "            skip_connections.append(out)\n",
    "            out = self.transDownBlocks[i](out)\n",
    "\n",
    "        out = self.bottleneck(out)\n",
    "        for i in range(len(self.up_blocks)):\n",
    "            skip = skip_connections.pop()\n",
    "            out = self.transUpBlocks[i](out, skip)\n",
    "            out = self.denseBlocksUp[i](out)\n",
    "\n",
    "        out = self.finalConv(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def FCDenseNet57(n_classes):\n",
    "    return FCDenseNet(\n",
    "        in_channels=3, down_blocks=(4, 4, 4, 4, 4),\n",
    "        up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4,\n",
    "        growth_rate=12, out_chans_first_conv=48, n_classes=n_classes)\n",
    "\n",
    "\n",
    "def FCDenseNet67(n_classes):\n",
    "    return FCDenseNet(\n",
    "        in_channels=3, down_blocks=(5, 5, 5, 5, 5),\n",
    "        up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n",
    "        growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n",
    "\n",
    "\n",
    "def FCDenseNet103(n_classes):\n",
    "    return FCDenseNet(\n",
    "        in_channels=3, down_blocks=(4,5,7,10,12),\n",
    "        up_blocks=(12,10,7,5,4), bottleneck_layers=15,\n",
    "        growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from . import imgs as img_utils\n",
    "\n",
    "RESULTS_PATH = '.results/'\n",
    "WEIGHTS_PATH = '.weights/'\n",
    "\n",
    "\n",
    "def save_weights(model, epoch, loss, err):\n",
    "    weights_fname = 'weights-%d-%.3f-%.3f.pth' % (epoch, loss, err)\n",
    "    weights_fpath = os.path.join(WEIGHTS_PATH, weights_fname)\n",
    "    torch.save({\n",
    "            'startEpoch': epoch,\n",
    "            'loss':loss,\n",
    "            'error': err,\n",
    "            'state_dict': model.state_dict()\n",
    "        }, weights_fpath)\n",
    "    shutil.copyfile(weights_fpath, WEIGHTS_PATH+'latest.th')\n",
    "\n",
    "def load_weights(model, fpath):\n",
    "    print(\"loading weights '{}'\".format(fpath))\n",
    "    weights = torch.load(fpath)\n",
    "    startEpoch = weights['startEpoch']\n",
    "    model.load_state_dict(weights['state_dict'])\n",
    "    print(\"loaded weights (lastEpoch {}, loss {}, error {})\"\n",
    "          .format(startEpoch-1, weights['loss'], weights['error']))\n",
    "    return startEpoch\n",
    "\n",
    "def get_predictions(output_batch):\n",
    "    bs,c,h,w = output_batch.size()\n",
    "    tensor = output_batch.data\n",
    "    values, indices = tensor.cpu().max(1)\n",
    "    indices = indices.view(bs,h,w)\n",
    "    return indices\n",
    "\n",
    "def error(preds, targets):\n",
    "    assert preds.size() == targets.size()\n",
    "    bs,h,w = preds.size()\n",
    "    n_pixels = bs*h*w\n",
    "    incorrect = preds.ne(targets).cpu().sum()\n",
    "    err = incorrect/n_pixels\n",
    "    return round(err,5)\n",
    "\n",
    "def train(model, trn_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    trn_loss = 0\n",
    "    trn_error = 0\n",
    "    for idx, data in enumerate(trn_loader):\n",
    "        inputs = Variable(data[0].cuda())\n",
    "        targets = Variable(data[1].cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trn_loss += loss.data[0]\n",
    "        pred = get_predictions(output)\n",
    "        trn_error += error(pred, targets.data.cpu())\n",
    "\n",
    "    trn_loss /= len(trn_loader)\n",
    "    trn_error /= len(trn_loader)\n",
    "    return trn_loss, trn_error\n",
    "\n",
    "def test(model, test_loader, criterion, epoch=1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_error = 0\n",
    "    for data, target in test_loader:\n",
    "        data = Variable(data.cuda(), volatile=True)\n",
    "        target = Variable(target.cuda())\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        pred = get_predictions(output)\n",
    "        test_error += error(pred, target.data.cpu())\n",
    "    test_loss /= len(test_loader)\n",
    "    test_error /= len(test_loader)\n",
    "    return test_loss, test_error\n",
    "\n",
    "def adjust_learning_rate(lr, decay, optimizer, cur_epoch, n_epochs):\n",
    "    \"\"\"Sets the learning rate to the initially\n",
    "        configured `lr` decayed by `decay` every `n_epochs`\"\"\"\n",
    "    new_lr = lr * (decay ** (cur_epoch // n_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform(m.weight)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "def predict(model, input_loader, n_batches=1):\n",
    "    input_loader.batch_size = 1\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for input, target in input_loader:\n",
    "        data = Variable(input.cuda(), volatile=True)\n",
    "        label = Variable(target.cuda())\n",
    "        output = model(data)\n",
    "        pred = get_predictions(output)\n",
    "        predictions.append([input,target,pred])\n",
    "    return predictions\n",
    "\n",
    "def view_sample_predictions(model, loader, n):\n",
    "    inputs, targets = next(iter(loader))\n",
    "    data = Variable(inputs.cuda(), volatile=True)\n",
    "    label = Variable(targets.cuda())\n",
    "    output = model(data)\n",
    "    pred = get_predictions(output)\n",
    "    batch_size = inputs.size(0)\n",
    "    for i in range(min(n, batch_size)):\n",
    "        img_utils.view_image(inputs[i])\n",
    "        img_utils.view_annotated(targets[i])\n",
    "        img_utils.view_annotated(pred[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Sky = [128,128,128]\n",
    "Building = [128,0,0]\n",
    "Pole = [192,192,128]\n",
    "Road = [128,64,128]\n",
    "Pavement = [60,40,222]\n",
    "Tree = [128,128,0]\n",
    "SignSymbol = [192,128,128]\n",
    "Fence = [64,64,128]\n",
    "Car = [64,0,128]\n",
    "Pedestrian = [64,64,0]\n",
    "Bicyclist = [0,128,192]\n",
    "Unlabelled = [0,0,0]\n",
    "\n",
    "DSET_MEAN = [0.41189489566336, 0.4251328133025, 0.4326707089857]\n",
    "DSET_STD = [0.27413549931506, 0.28506257482912, 0.28284674400252]\n",
    "\n",
    "label_colours = np.array([Sky, Building, Pole, Road, Pavement,\n",
    "      Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n",
    "\n",
    "\n",
    "def view_annotated(tensor, plot=True):\n",
    "    temp = tensor.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0,11):\n",
    "        r[temp==l]=label_colours[l,0]\n",
    "        g[temp==l]=label_colours[l,1]\n",
    "        b[temp==l]=label_colours[l,2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:,:,0] = (r/255.0)#[:,:,0]\n",
    "    rgb[:,:,1] = (g/255.0)#[:,:,1]\n",
    "    rgb[:,:,2] = (b/255.0)#[:,:,2]\n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "\n",
    "def decode_image(tensor):\n",
    "    inp = tensor.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array(DSET_MEAN)\n",
    "    std = np.array(DSET_STD)\n",
    "    inp = std * inp + mean\n",
    "    return inp\n",
    "\n",
    "def view_image(tensor):\n",
    "    inp = decode_image(tensor)\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.datasets.folder import is_image_file, default_loader\n",
    "\n",
    "\n",
    "classes = ['Sky', 'Building', 'Column-Pole', 'Road',\n",
    "           'Sidewalk', 'Tree', 'Sign-Symbol', 'Fence', 'Car', 'Pedestrain',\n",
    "           'Bicyclist', 'Void']\n",
    "\n",
    "# https://github.com/yandex/segnet-torch/blob/master/datasets/camvid-gen.lua\n",
    "class_weight = torch.FloatTensor([\n",
    "    0.58872014284134, 0.51052379608154, 2.6966278553009,\n",
    "    0.45021694898605, 1.1785038709641, 0.77028578519821, 2.4782588481903,\n",
    "    2.5273461341858, 1.0122526884079, 3.2375309467316, 4.1312313079834, 0])\n",
    "\n",
    "mean = [0.41189489566336, 0.4251328133025, 0.4326707089857]\n",
    "std = [0.27413549931506, 0.28506257482912, 0.28284674400252]\n",
    "\n",
    "class_color = [\n",
    "    (128, 128, 128),\n",
    "    (128, 0, 0),\n",
    "    (192, 192, 128),\n",
    "    (128, 64, 128),\n",
    "    (0, 0, 192),\n",
    "    (128, 128, 0),\n",
    "    (192, 128, 128),\n",
    "    (64, 64, 128),\n",
    "    (64, 0, 128),\n",
    "    (64, 64, 0),\n",
    "    (0, 128, 192),\n",
    "    (0, 0, 0),\n",
    "]\n",
    "\n",
    "\n",
    "def _make_dataset(dir):\n",
    "    images = []\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                item = path\n",
    "                images.append(item)\n",
    "    return images\n",
    "\n",
    "\n",
    "class LabelToLongTensor(object):\n",
    "    def __call__(self, pic):\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            label = torch.from_numpy(pic).long()\n",
    "        else:\n",
    "            label = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "            label = label.view(pic.size[1], pic.size[0], 1)\n",
    "            label = label.transpose(0, 1).transpose(0, 2).squeeze().contiguous().long()\n",
    "        return label\n",
    "\n",
    "\n",
    "class LabelTensorToPILImage(object):\n",
    "    def __call__(self, label):\n",
    "        label = label.unsqueeze(0)\n",
    "        colored_label = torch.zeros(3, label.size(1), label.size(2)).byte()\n",
    "        for i, color in enumerate(class_color):\n",
    "            mask = label.eq(i)\n",
    "            for j in range(3):\n",
    "                colored_label[j].masked_fill_(mask, color[j])\n",
    "        npimg = colored_label.numpy()\n",
    "        npimg = np.transpose(npimg, (1, 2, 0))\n",
    "        mode = None\n",
    "        if npimg.shape[2] == 1:\n",
    "            npimg = npimg[:, :, 0]\n",
    "            mode = \"L\"\n",
    "\n",
    "        return Image.fromarray(npimg, mode=mode)\n",
    "\n",
    "\n",
    "class CamVid(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, split='train', joint_transform=None,\n",
    "                 transform=None, target_transform=LabelToLongTensor(),\n",
    "                 download=False,\n",
    "                 loader=default_loader):\n",
    "        self.root = root\n",
    "        assert split in ('train', 'val', 'test')\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.joint_transform = joint_transform\n",
    "        self.loader = loader\n",
    "        self.class_weight = class_weight\n",
    "        self.classes = classes\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        self.imgs = _make_dataset(os.path.join(self.root, self.split))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        target = Image.open(path.replace(self.split, self.split + 'annot'))\n",
    "\n",
    "        if self.joint_transform is not None:\n",
    "            img, target = self.joint_transform([img, target])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def download(self):\n",
    "        # TODO: please download the dataset from\n",
    "        # https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import tiramisu\n",
    "from datasets import camvid\n",
    "from datasets import joint_transforms\n",
    "import utils.imgs\n",
    "import utils.training as train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train: %d\" %len(train_loader.dataset.imgs))\n",
    "print(\"Val: %d\" %len(val_loader.dataset.imgs))\n",
    "print(\"Test: %d\" %len(test_loader.dataset.imgs))\n",
    "print(\"Classes: %d\" % len(train_loader.dataset.classes))\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "print(\"Inputs: \", inputs.size())\n",
    "print(\"Targets: \", targets.size())\n",
    "\n",
    "utils.imgs.view_image(inputs[0])\n",
    "utils.imgs.view_annotated(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "LR_DECAY = 0.995\n",
    "DECAY_EVERY_N_EPOCHS = 1\n",
    "N_EPOCHS = 2\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tiramisu.FCDenseNet67(n_classes=12).cuda()\n",
    "model.apply(train_utils.weights_init)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "criterion = nn.NLLLoss2d(weight=camvid.class_weight.cuda()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    since = time.time()\n",
    "\n",
    "    ### Train ###\n",
    "    trn_loss, trn_err = train_utils.train(\n",
    "        model, train_loader, optimizer, criterion, epoch)\n",
    "    print('Epoch {:d}\\nTrain - Loss: {:.4f}, Acc: {:.4f}'.format(\n",
    "        epoch, trn_loss, 1-trn_err))    \n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Train Time {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    ### Test ###\n",
    "    val_loss, val_err = train_utils.test(model, val_loader, criterion, epoch)    \n",
    "    print('Val - Loss: {:.4f} | Acc: {:.4f}'.format(val_loss, 1-val_err))\n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Total Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    ### Checkpoint ###    \n",
    "    train_utils.save_weights(model, epoch, val_loss, val_err)\n",
    "\n",
    "    ### Adjust Lr ###\n",
    "    train_utils.adjust_learning_rate(LR, LR_DECAY, optimizer, \n",
    "                                     epoch, DECAY_EVERY_N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utils.test(model, test_loader, criterion, epoch=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utils.view_sample_predictions(model, test_loader, n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
